import json
import os
import time
import random
from dotenv import load_dotenv
from openai import OpenAI
from supabase import create_client, Client

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv('.env.local')

SUPABASE_URL = os.getenv("SUPABASE_URL") or os.getenv("NEXT_PUBLIC_SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY_FOR_EMBEDDING") or os.getenv("OPENAI_API_KEY")

if not all([SUPABASE_URL, SUPABASE_KEY, OPENAI_API_KEY]):
    raise ValueError("è¯·ç¡®ä¿ .env.local æ–‡ä»¶ä¸­å·²è®¾ç½®æ‰€æœ‰å¿…éœ€çš„ç¯å¢ƒå˜é‡ã€‚")

# åˆå§‹åŒ–å®¢æˆ·ç«¯
print("æ­£åœ¨åˆå§‹åŒ– Supabase å’Œ OpenAI å®¢æˆ·ç«¯...")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
openai_client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url="https://api.openai.com/v1",
)
print("å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼")

def find_node_by_id(node_id, all_nodes):
    """æ ¹æ®IDæŸ¥æ‰¾èŠ‚ç‚¹"""
    for node in all_nodes:
        if node.get("id") == node_id:
            return node
    return None

def generate_base_node_text(node):
    """ç”ŸæˆèŠ‚ç‚¹çš„åŸºç¡€æ–‡æœ¬æè¿°"""
    props = node.get("properties", {})
    name = props.get("name")
    description = props.get("description")
    labels = node.get("labels", ["Thing"])
    label = labels[0] if labels else "Thing"
    
    # æ ¹æ®æ ‡ç­¾ç±»å‹ç”Ÿæˆæ›´ç²¾ç¡®çš„æè¿°
    if label == "Module":
        if name and description:
            return f"è™šå¹»å¼•æ“æ¨¡å—'{name}': {description}"
        elif name:
            return f"è™šå¹»å¼•æ“æ¨¡å—'{name}'ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ¨¡å—ã€‚"
    elif label == "System":
        if name and description:
            return f"è™šå¹»å¼•æ“ç³»ç»Ÿ'{name}': {description}"
        elif name:
            return f"è™šå¹»å¼•æ“ç³»ç»Ÿ'{name}'ï¼Œè¿™æ˜¯ä¸€ä¸ªç³»ç»Ÿçº§ç»„ä»¶ã€‚"
    elif label == "Class":
        if name and description:
            return f"è™šå¹»å¼•æ“ç±»'{name}': {description}"
        elif name:
            return f"è™šå¹»å¼•æ“ç±»'{name}'ï¼Œè¿™æ˜¯ä¸€ä¸ªç±»å®šä¹‰ã€‚"
    elif label == "Interface":
        if name and description:
            return f"è™šå¹»å¼•æ“æ¥å£'{name}': {description}"
        elif name:
            return f"è™šå¹»å¼•æ“æ¥å£'{name}'ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¥å£å®šä¹‰ã€‚"
    elif label == "Subsystem":
        if name and description:
            return f"è™šå¹»å¼•æ“å­ç³»ç»Ÿ'{name}': {description}"
        elif name:
            return f"è™šå¹»å¼•æ“å­ç³»ç»Ÿ'{name}'ï¼Œè¿™æ˜¯ä¸€ä¸ªå­ç³»ç»Ÿç»„ä»¶ã€‚"
    else:
        if name and description:
            return f"å…³äºè™šå¹»å¼•æ“çš„'{name}' ({label}): {description}"
        elif name:
            return f"è™šå¹»å¼•æ“ä¸­æœ‰ä¸€ä¸ªæ¦‚å¿µæˆ–åŠŸèƒ½å«åš'{name}' ({label})ã€‚"
        else:
            return None

def truncate_text_for_embedding(text, max_tokens=4000):
    """æˆªæ–­æ–‡æœ¬ä»¥é€‚åº”embeddingæ¨¡å‹çš„tokené™åˆ¶"""
    # æ›´ä¿å®ˆçš„ä¼°ç®—ï¼š1ä¸ªtokençº¦ç­‰äº2.5ä¸ªå­—ç¬¦ï¼ˆä¸­æ–‡æ›´å¯†é›†ï¼‰
    max_chars = max_tokens * 2.5
    
    if len(text) <= max_chars:
        return text
    
    # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œå°è¯•æ™ºèƒ½æˆªæ–­
    print(f"    -> æ–‡æœ¬è¿‡é•¿ ({len(text)} å­—ç¬¦)ï¼Œå¼€å§‹æ™ºèƒ½æˆªæ–­...")
    
    # é¦–å…ˆå°è¯•åœ¨å¥å·å¤„æˆªæ–­
    truncated_text = text[:max_chars]
    last_period = truncated_text.rfind('ã€‚')
    
    if last_period > max_chars * 0.8:  # å¦‚æœå¥å·ä½ç½®åˆç†
        truncated_text = truncated_text[:last_period + 1]
        print(f"    -> åœ¨å¥å·å¤„æˆªæ–­ï¼Œä¿ç•™ {len(truncated_text)} å­—ç¬¦")
    else:
        # å¦‚æœæ²¡æœ‰åˆé€‚çš„å¥å·ï¼Œå°è¯•åœ¨é€—å·å¤„æˆªæ–­
        last_comma = truncated_text.rfind('ï¼Œ')
        if last_comma > max_chars * 0.8:
            truncated_text = truncated_text[:last_comma + 1] + "ç­‰ã€‚"
            print(f"    -> åœ¨é€—å·å¤„æˆªæ–­å¹¶æ·»åŠ 'ç­‰'ï¼Œä¿ç•™ {len(truncated_text)} å­—ç¬¦")
        else:
            # æœ€åé€‰æ‹©åœ¨å­—ç¬¦é™åˆ¶å¤„æˆªæ–­
            truncated_text = text[:max_chars - 3] + "..."
            print(f"    -> å¼ºåˆ¶æˆªæ–­å¹¶æ·»åŠ çœç•¥å·ï¼Œä¿ç•™ {len(truncated_text)} å­—ç¬¦")
    
    return truncated_text

def enhance_node_with_relationships(node, all_relationships, all_nodes):
    """ä¸ºèŠ‚ç‚¹æ·»åŠ å…³ç³»ä¿¡æ¯ï¼Œç”Ÿæˆå¢å¼ºçš„æ–‡æœ¬æè¿°"""
    base_text = generate_base_node_text(node)
    if not base_text:
        return None
    
    node_id = node.get("id")
    dependencies = []
    dependents = []
    hierarchy_info = []
    
    # æŸ¥æ‰¾ä¸æ­¤èŠ‚ç‚¹ç›¸å…³çš„å…³ç³»
    for rel in all_relationships:
        if rel.get("start", {}).get("id") == node_id:
            # å½“å‰èŠ‚ç‚¹æ˜¯å…³ç³»çš„èµ·å§‹ç‚¹ï¼ˆä¾èµ–å…¶ä»–æ¨¡å—ï¼‰
            target_id = rel.get("end", {}).get("id")
            rel_type = rel.get("label")
            rel_props = rel.get("properties", {})
            dependency_type = rel_props.get("type", "")
            target_node = find_node_by_id(target_id, all_nodes)
            if target_node:
                target_name = target_node.get("properties", {}).get("name")
                target_labels = target_node.get("labels", [])
                if target_name:
                    # æ ¹æ®ä¾èµ–ç±»å‹ç”Ÿæˆæ›´ç²¾ç¡®çš„æè¿°
                    if dependency_type == "PublicDependencyModuleNames":
                        dependencies.append(f"å…¬å¼€ä¾èµ–'{target_name}'")
                    elif dependency_type == "PrivateDependencyModuleNames":
                        dependencies.append(f"ç§æœ‰ä¾èµ–'{target_name}'")
                    elif dependency_type == "PublicIncludePathModuleNames":
                        dependencies.append(f"å…¬å¼€åŒ…å«è·¯å¾„ä¾èµ–'{target_name}'")
                    elif dependency_type == "PrivateIncludePathModuleNames":
                        dependencies.append(f"ç§æœ‰åŒ…å«è·¯å¾„ä¾èµ–'{target_name}'")
                    else:
                        dependencies.append(f"ä¾èµ–'{target_name}'")
                    
        elif rel.get("end", {}).get("id") == node_id:
            # å½“å‰èŠ‚ç‚¹æ˜¯å…³ç³»çš„ç»ˆæ­¢ç‚¹ï¼ˆè¢«å…¶ä»–æ¨¡å—ä¾èµ–ï¼‰
            source_id = rel.get("start", {}).get("id")
            rel_type = rel.get("label")
            rel_props = rel.get("properties", {})
            dependency_type = rel_props.get("type", "")
            source_node = find_node_by_id(source_id, all_nodes)
            if source_node:
                source_name = source_node.get("properties", {}).get("name")
                source_labels = source_node.get("labels", [])
                if source_name:
                    # æ ¹æ®ä¾èµ–ç±»å‹ç”Ÿæˆæ›´ç²¾ç¡®çš„æè¿°
                    if dependency_type == "PublicDependencyModuleNames":
                        dependents.append(f"è¢«'{source_name}'å…¬å¼€ä¾èµ–")
                    elif dependency_type == "PrivateDependencyModuleNames":
                        dependents.append(f"è¢«'{source_name}'ç§æœ‰ä¾èµ–")
                    elif dependency_type == "PublicIncludePathModuleNames":
                        dependents.append(f"è¢«'{source_name}'å…¬å¼€åŒ…å«è·¯å¾„ä¾èµ–")
                    elif dependency_type == "PrivateIncludePathModuleNames":
                        dependents.append(f"è¢«'{source_name}'ç§æœ‰åŒ…å«è·¯å¾„ä¾èµ–")
                    else:
                        dependents.append(f"è¢«'{source_name}'ä¾èµ–")
    
    # é™åˆ¶ä¾èµ–å…³ç³»æ•°é‡ï¼Œé¿å…æ–‡æœ¬è¿‡é•¿
    max_dependencies = 50
    max_dependents = 30
    
    if len(dependencies) > max_dependencies:
        dependencies = dependencies[:max_dependencies]
        dependencies.append(f"ç­‰{len(dependencies)}ä¸ªä¾èµ–")
    
    if len(dependents) > max_dependents:
        dependents = dependents[:max_dependents]
        dependents.append(f"ç­‰{len(dependents)}ä¸ªè¢«ä¾èµ–")
    
    # æ·»åŠ å±‚æ¬¡ç»“æ„ä¿¡æ¯
    node_labels = node.get("labels", [])
    if "System" in node_labels:
        hierarchy_info.append("è¿™æ˜¯ä¸€ä¸ªç³»ç»Ÿçº§ç»„ä»¶")
    elif "Subsystem" in node_labels:
        hierarchy_info.append("è¿™æ˜¯ä¸€ä¸ªå­ç³»ç»Ÿç»„ä»¶")
    elif "Module" in node_labels:
        hierarchy_info.append("è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ¨¡å—")
    elif "Class" in node_labels:
        hierarchy_info.append("è¿™æ˜¯ä¸€ä¸ªç±»å®šä¹‰")
    elif "Interface" in node_labels:
        hierarchy_info.append("è¿™æ˜¯ä¸€ä¸ªæ¥å£å®šä¹‰")
    
    # ç»„åˆæœ€ç»ˆæ–‡æœ¬
    relationship_text = []
    if dependencies:
        relationship_text.append("å®ƒ" + "ï¼Œ".join(dependencies))
    if dependents:
        relationship_text.append("ï¼Œ".join(dependents))
    if hierarchy_info:
        relationship_text.append("ï¼Œ".join(hierarchy_info))
    
    if relationship_text:
        base_text += " " + "ï¼Œ".join(relationship_text) + "ã€‚"
    
    # æˆªæ–­æ–‡æœ¬ä»¥é€‚åº”tokené™åˆ¶
    final_text = truncate_text_for_embedding(base_text)
    
    return final_text

def generate_relationship_text(relationship, all_nodes):
    """ä¸ºå…³ç³»ç”Ÿæˆç‹¬ç«‹çš„æè¿°æ–‡æœ¬"""
    rel_type = relationship.get("label")
    rel_props = relationship.get("properties", {})
    dependency_type = rel_props.get("type", "")
    start_node = relationship.get("start", {})
    end_node = relationship.get("end", {})
    
    start_name = start_node.get("properties", {}).get("name")
    end_name = end_node.get("properties", {}).get("name")
    
    if start_name and end_name:
        # æ ¹æ®ä¾èµ–ç±»å‹ç”Ÿæˆæ›´ç²¾ç¡®çš„æè¿°
        if dependency_type == "PublicDependencyModuleNames":
            return f"åœ¨è™šå¹»å¼•æ“ä¸­ï¼Œ'{start_name}' å…¬å¼€ä¾èµ– '{end_name}'ï¼Œè¿™æ„å‘³ç€'{start_name}'å¯ä»¥è®¿é—®'{end_name}'çš„å…¬å…±æ¥å£ã€‚"
        elif dependency_type == "PrivateDependencyModuleNames":
            return f"åœ¨è™šå¹»å¼•æ“ä¸­ï¼Œ'{start_name}' ç§æœ‰ä¾èµ– '{end_name}'ï¼Œè¿™æ„å‘³ç€'{start_name}'å¯ä»¥è®¿é—®'{end_name}'çš„å†…éƒ¨å®ç°ã€‚"
        elif dependency_type == "PublicIncludePathModuleNames":
            return f"åœ¨è™šå¹»å¼•æ“ä¸­ï¼Œ'{start_name}' å…¬å¼€åŒ…å«è·¯å¾„ä¾èµ– '{end_name}'ï¼Œè¿™æ„å‘³ç€'{start_name}'å¯ä»¥åŒ…å«'{end_name}'çš„å…¬å…±å¤´æ–‡ä»¶ã€‚"
        elif dependency_type == "PrivateIncludePathModuleNames":
            return f"åœ¨è™šå¹»å¼•æ“ä¸­ï¼Œ'{start_name}' ç§æœ‰åŒ…å«è·¯å¾„ä¾èµ– '{end_name}'ï¼Œè¿™æ„å‘³ç€'{start_name}'å¯ä»¥åŒ…å«'{end_name}'çš„å†…éƒ¨å¤´æ–‡ä»¶ã€‚"
        else:
            return f"åœ¨è™šå¹»å¼•æ“ä¸­ï¼Œ'{start_name}' {rel_type} '{end_name}'ã€‚"
    
    return None

def generate_hierarchy_and_type_documents(all_nodes, all_relationships):
    """ç”Ÿæˆå±‚æ¬¡ç»“æ„å’Œç±»å‹å…³ç³»çš„é¢å¤–æ–‡æ¡£"""
    documents = []
    
    # æŒ‰æ ‡ç­¾åˆ†ç»„èŠ‚ç‚¹
    nodes_by_label = {}
    for node in all_nodes:
        labels = node.get("labels", [])
        for label in labels:
            if label not in nodes_by_label:
                nodes_by_label[label] = []
            nodes_by_label[label].append(node)
    
    # ç”Ÿæˆå±‚æ¬¡ç»“æ„æ–‡æ¡£
    if "System" in nodes_by_label:
        system_nodes = nodes_by_label["System"]
        if len(system_nodes) > 0:
            system_names = [node.get("properties", {}).get("name", "æœªçŸ¥") for node in system_nodes if node.get("properties", {}).get("name")]
            if system_names:
                hierarchy_text = f"è™šå¹»å¼•æ“åŒ…å«ä»¥ä¸‹ç³»ç»Ÿçº§ç»„ä»¶: {', '.join(system_names)}ã€‚è¿™äº›ç³»ç»Ÿæ˜¯å¼•æ“çš„æ ¸å¿ƒæ¶æ„ç»„ä»¶ï¼Œè´Ÿè´£ç®¡ç†ä¸åŒçš„åŠŸèƒ½é¢†åŸŸã€‚"
                documents.append(hierarchy_text)
    
    if "Subsystem" in nodes_by_label:
        subsystem_nodes = nodes_by_label["Subsystem"]
        if len(subsystem_nodes) > 0:
            subsystem_names = [node.get("properties", {}).get("name", "æœªçŸ¥") for node in subsystem_nodes if node.get("properties", {}).get("name")]
            if subsystem_names:
                hierarchy_text = f"è™šå¹»å¼•æ“åŒ…å«ä»¥ä¸‹å­ç³»ç»Ÿç»„ä»¶: {', '.join(subsystem_names)}ã€‚è¿™äº›å­ç³»ç»Ÿæ˜¯ç³»ç»Ÿçº§ç»„ä»¶çš„å­ç»„ä»¶ï¼Œæä¾›æ›´ç»†ç²’åº¦çš„åŠŸèƒ½ã€‚"
                documents.append(hierarchy_text)
    
    if "Module" in nodes_by_label:
        module_nodes = nodes_by_label["Module"]
        if len(module_nodes) > 0:
            module_names = [node.get("properties", {}).get("name", "æœªçŸ¥") for node in module_nodes if node.get("properties", {}).get("name")]
            if module_names:
                hierarchy_text = f"è™šå¹»å¼•æ“åŒ…å«ä»¥ä¸‹åŠŸèƒ½æ¨¡å—: {', '.join(module_names[:20])}{'...' if len(module_names) > 20 else ''}ã€‚è¿™äº›æ¨¡å—æ˜¯å¼•æ“çš„åŠŸèƒ½å•å…ƒï¼Œæ¯ä¸ªæ¨¡å—è´Ÿè´£ç‰¹å®šçš„åŠŸèƒ½é¢†åŸŸã€‚"
                documents.append(hierarchy_text)
    
    # ç”Ÿæˆç±»å‹å…³ç³»æ–‡æ¡£
    if "Class" in nodes_by_label:
        class_nodes = nodes_by_label["Class"]
        if len(class_nodes) > 0:
            class_names = [node.get("properties", {}).get("name", "æœªçŸ¥") for node in class_nodes if node.get("properties", {}).get("name")]
            if class_names:
                type_text = f"è™šå¹»å¼•æ“åŒ…å«ä»¥ä¸‹ç±»å®šä¹‰: {', '.join(class_names[:20])}{'...' if len(class_names) > 20 else ''}ã€‚è¿™äº›ç±»æä¾›äº†å¼•æ“çš„é¢å‘å¯¹è±¡ç¼–ç¨‹æ¥å£ã€‚"
                documents.append(type_text)
    
    if "Interface" in nodes_by_label:
        interface_nodes = nodes_by_label["Interface"]
        if len(interface_nodes) > 0:
            interface_names = [node.get("properties", {}).get("name", "æœªçŸ¥") for node in interface_nodes if node.get("properties", {}).get("name")]
            if interface_names:
                type_text = f"è™šå¹»å¼•æ“åŒ…å«ä»¥ä¸‹æ¥å£å®šä¹‰: {', '.join(interface_names)}ã€‚è¿™äº›æ¥å£å®šä¹‰äº†ç±»ä¹‹é—´çš„å¥‘çº¦å’ŒæŠ½è±¡ã€‚"
                documents.append(type_text)
    
    return documents

def create_embedding_with_retry(text, max_retries=3, base_delay=2):
    """å¸¦é‡è¯•æœºåˆ¶çš„å‘é‡ç”Ÿæˆ"""
    # ç¡®ä¿æ–‡æœ¬é•¿åº¦åœ¨åˆç†èŒƒå›´å†…
    text = truncate_text_for_embedding(text)
    
    for attempt in range(max_retries):
        try:
            response = openai_client.embeddings.create(
                input=text,
                model="text-embedding-3-small"
            )
            return response.data[0].embedding
        except Exception as e:
            if attempt < max_retries - 1:
                delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                print(f"    -> é‡è¯• {attempt + 1}/{max_retries}ï¼Œç­‰å¾… {delay:.1f} ç§’...")
                time.sleep(delay)
            else:
                raise e

def save_progress(progress_file, data):
    """ä¿å­˜è¿›åº¦"""
    try:
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜åˆ° {progress_file}")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜è¿›åº¦æ—¶å‘ç”Ÿé”™è¯¯: {e}")

def load_progress(progress_file):
    """åŠ è½½è¿›åº¦"""
    if os.path.exists(progress_file):
        try:
            with open(progress_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"ğŸ“‚ ä» {progress_file} åŠ è½½äº†ä¹‹å‰çš„è¿›åº¦")
            return data
        except Exception as e:
            print(f"âš ï¸ åŠ è½½è¿›åº¦æ—¶å‘ç”Ÿé”™è¯¯: {e}ï¼Œå°†é‡æ–°å¼€å§‹")
    return {"processed_nodes": [], "processed_relationships": [], "documents": [], "current_phase": "nodes"}

def migrate_data(json_file_path):
    print(f"å¼€å§‹å¤„ç†JSON Linesæ–‡ä»¶: {json_file_path}")
    
    # è¿›åº¦æ–‡ä»¶
    progress_file = "migration_progress.json"
    
    # åŠ è½½ä¹‹å‰çš„è¿›åº¦
    progress = load_progress(progress_file)
    processed_nodes = set(progress.get("processed_nodes", []))
    processed_relationships = set(progress.get("processed_relationships", []))
    documents_to_insert = progress.get("documents", [])
    current_phase = progress.get("current_phase", "nodes")
    
    print(f"ğŸ“Š å·²å¤„ç†èŠ‚ç‚¹: {len(processed_nodes)}")
    print(f"ğŸ“Š å·²å¤„ç†å…³ç³»: {len(processed_relationships)}")
    print(f"ğŸ“Š å·²ç”Ÿæˆæ–‡æ¡£: {len(documents_to_insert)}")
    print(f"ğŸ“Š å½“å‰é˜¶æ®µ: {current_phase}")
    
    # å­˜å‚¨æ‰€æœ‰æ•°æ®
    all_nodes = []
    all_relationships = []
    
    # ç¬¬ä¸€æ­¥ï¼šè¯»å–æ‰€æœ‰æ•°æ®
    print("æ­£åœ¨è¯»å–èŠ‚ç‚¹å’Œå…³ç³»æ•°æ®...")
    with open(json_file_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                item = json.loads(line)
                if item.get("type") == "node":
                    all_nodes.append(item)
                elif item.get("type") == "relationship":
                    all_relationships.append(item)
            except json.JSONDecodeError:
                continue
    
    print(f"è¯»å–åˆ° {len(all_nodes)} ä¸ªèŠ‚ç‚¹å’Œ {len(all_relationships)} ä¸ªå…³ç³»")
    
    # ç¬¬äºŒæ­¥ï¼šå¤„ç†èŠ‚ç‚¹ï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ…å«å…³ç³»ä¿¡æ¯ï¼‰
    if current_phase in ["nodes", "start"]:
        print("\næ­£åœ¨å¤„ç†èŠ‚ç‚¹ï¼ˆåŒ…å«å…³ç³»ä¿¡æ¯ï¼‰...")
        node_count = 0
        for node in all_nodes:
            node_count += 1
            name = node.get("properties", {}).get("name")
            node_id = node.get("id")
            
            if not name or node_id in processed_nodes:
                continue
                
            # ç”Ÿæˆå¢å¼ºçš„èŠ‚ç‚¹æ–‡æœ¬
            enhanced_text = enhance_node_with_relationships(node, all_relationships, all_nodes)
            if not enhanced_text:
                continue
            
            print(f"[{node_count}/{len(all_nodes)}] æ­£åœ¨å¤„ç†èŠ‚ç‚¹ '{name}'...")
            
            # ç”Ÿæˆå‘é‡å¹¶å‡†å¤‡æ’å…¥
            try:
                embedding = create_embedding_with_retry(enhanced_text)
                documents_to_insert.append({'content': enhanced_text, 'embedding': embedding})
                processed_nodes.add(node_id)
                
                # æ¯å¤„ç†5ä¸ªèŠ‚ç‚¹ä¿å­˜ä¸€æ¬¡è¿›åº¦
                if len(processed_nodes) % 5 == 0:
                    save_progress(progress_file, {
                        "processed_nodes": list(processed_nodes),
                        "processed_relationships": list(processed_relationships),
                        "documents": documents_to_insert,
                        "current_phase": "nodes"
                    })
                    print(f"ğŸ’¾ å·²ä¿å­˜è¿›åº¦ï¼Œå·²å¤„ç† {len(processed_nodes)} ä¸ªèŠ‚ç‚¹")
                
                # éšæœºå»¶è¿Ÿï¼Œé¿å…APIé™åˆ¶
                time.sleep(random.uniform(0.5, 1.5))
                
            except Exception as e:
                print(f"    -> ERROR: å¤„ç†èŠ‚ç‚¹ '{name}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                # ä¿å­˜å½“å‰è¿›åº¦
                save_progress(progress_file, {
                    "processed_nodes": list(processed_nodes),
                    "processed_relationships": list(processed_relationships),
                    "documents": documents_to_insert,
                    "current_phase": "nodes"
                })
                print("ğŸ”„ è¿›åº¦å·²ä¿å­˜ï¼Œæ‚¨å¯ä»¥é‡æ–°è¿è¡Œè„šæœ¬ç»§ç»­å¤„ç†")
                return
        
        print(f"âœ… èŠ‚ç‚¹å¤„ç†å®Œæˆï¼æ€»å…±å¤„ç†äº† {len(processed_nodes)} ä¸ªèŠ‚ç‚¹")
        current_phase = "relationships"
    
    # ç¬¬ä¸‰æ­¥ï¼šå¤„ç†é‡è¦å…³ç³»ï¼ˆç”Ÿæˆç‹¬ç«‹çš„å…³ç³»æè¿°æ–‡æœ¬ï¼‰
    if current_phase in ["relationships", "nodes"]:
        print("\næ­£åœ¨å¤„ç†é‡è¦å…³ç³»...")
        rel_count = 0
        processed_rel_types = set()
        
        for rel in all_relationships:
            rel_type = rel.get("label")
            rel_id = rel.get("id")
            
            if rel_type == "DEPENDS_ON" and rel_id not in processed_relationships:
                rel_count += 1
                rel_text = generate_relationship_text(rel, all_nodes)
                
                if rel_text:
                    start_name = rel.get("start", {}).get("properties", {}).get("name", "æœªçŸ¥")
                    end_name = rel.get("end", {}).get("properties", {}).get("name", "æœªçŸ¥")
                    rel_props = rel.get("properties", {})
                    dependency_type = rel_props.get("type", "")
                    processed_rel_types.add(dependency_type)
                    
                    print(f"[å…³ç³» {rel_count}] æ­£åœ¨å¤„ç†å…³ç³» '{start_name}' {rel_type} '{end_name}' (ç±»å‹: {dependency_type})...")
                    
                    try:
                        embedding = create_embedding_with_retry(rel_text)
                        documents_to_insert.append({'content': rel_text, 'embedding': embedding})
                        processed_relationships.add(rel_id)
                        
                        # æ¯å¤„ç†20ä¸ªå…³ç³»ä¿å­˜ä¸€æ¬¡è¿›åº¦
                        if len(processed_relationships) % 20 == 0:
                            save_progress(progress_file, {
                                "processed_nodes": list(processed_nodes),
                                "processed_relationships": list(processed_relationships),
                                "documents": documents_to_insert,
                                "current_phase": "relationships"
                            })
                            print(f"ğŸ’¾ å·²ä¿å­˜è¿›åº¦ï¼Œå·²å¤„ç† {len(processed_relationships)} ä¸ªå…³ç³»")
                        
                        # éšæœºå»¶è¿Ÿï¼Œé¿å…APIé™åˆ¶
                        time.sleep(random.uniform(0.3, 0.8))
                        
                    except Exception as e:
                        print(f"    -> ERROR: å¤„ç†å…³ç³»æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        # ä¿å­˜å½“å‰è¿›åº¦
                        save_progress(progress_file, {
                            "processed_nodes": list(processed_nodes),
                            "processed_relationships": list(processed_relationships),
                            "documents": documents_to_insert,
                            "current_phase": "relationships"
                        })
                        print("ğŸ”„ è¿›åº¦å·²ä¿å­˜ï¼Œæ‚¨å¯ä»¥é‡æ–°è¿è¡Œè„šæœ¬ç»§ç»­å¤„ç†")
                        return
        
        print(f"âœ… å…³ç³»å¤„ç†å®Œæˆï¼æ€»å…±å¤„ç†äº† {len(processed_relationships)} ä¸ªDEPENDS_ONå…³ç³»")
        print(f"ğŸ“Š å‘ç°çš„å…³ç³»ç±»å‹: {', '.join(processed_rel_types)}")
        current_phase = "hierarchy"
    
    # ç¬¬å››æ­¥ï¼šç”Ÿæˆå±‚æ¬¡ç»“æ„å’Œç±»å‹å…³ç³»æ–‡æ¡£
    if current_phase in ["hierarchy", "relationships"]:
        print("\næ­£åœ¨ç”Ÿæˆå±‚æ¬¡ç»“æ„å’Œç±»å‹å…³ç³»æ–‡æ¡£...")
        hierarchy_docs = generate_hierarchy_and_type_documents(all_nodes, all_relationships)
        
        for i, doc in enumerate(hierarchy_docs):
            try:
                embedding = create_embedding_with_retry(doc)
                documents_to_insert.append({'content': doc, 'embedding': embedding})
                print(f"  ç”Ÿæˆå±‚æ¬¡ç»“æ„æ–‡æ¡£ {i+1}/{len(hierarchy_docs)}: {doc[:100]}...")
                time.sleep(random.uniform(0.5, 1.0))
            except Exception as e:
                print(f"    -> ERROR: ç”Ÿæˆå±‚æ¬¡ç»“æ„æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        
        print(f"âœ… ç”Ÿæˆäº† {len(hierarchy_docs)} ä¸ªå±‚æ¬¡ç»“æ„å’Œç±»å‹å…³ç³»æ–‡æ¡£")
        current_phase = "insert"
    
    # ç¬¬äº”æ­¥ï¼šåˆ†æ‰¹æ’å…¥æ‰€æœ‰æ•°æ®
    if current_phase in ["insert", "hierarchy"] and documents_to_insert:
        print(f"\nä»»åŠ¡å®Œæˆ! å‡†å¤‡å°† {len(documents_to_insert)} ä¸ªæ–‡æ¡£åˆ†æ‰¹æ’å…¥åˆ°Supabase...")
        
        # åˆ†æ‰¹å¤§å°
        batch_size = 20  # è¿›ä¸€æ­¥å‡å°æ‰¹æ¬¡å¤§å°
        total_inserted = 0
        
        try:
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(documents_to_insert), batch_size):
                batch = documents_to_insert[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                total_batches = (len(documents_to_insert) + batch_size - 1) // batch_size
                
                print(f"æ­£åœ¨æ’å…¥ç¬¬ {batch_num}/{total_batches} æ‰¹ ({len(batch)} æ¡è®°å½•)...")
                
                response = supabase.table('ue_documents').insert(batch).execute()
                
                if response.data:
                    total_inserted += len(response.data)
                    print(f"âœ… ç¬¬ {batch_num} æ‰¹æ’å…¥æˆåŠŸï¼å·²ç´¯è®¡æ’å…¥ {total_inserted} æ¡è®°å½•")
                else:
                    print(f"âš ï¸ ç¬¬ {batch_num} æ‰¹æ’å…¥å®Œæˆï¼Œä½†æœªè¿”å›æ•°æ®")
                    if hasattr(response, 'error') and response.error:
                        print(f"é”™è¯¯ä¿¡æ¯: {response.error}")
                
                # åœ¨æ‰¹æ¬¡ä¹‹é—´ç¨ä½œåœé¡¿
                if i + batch_size < len(documents_to_insert):
                    time.sleep(3)  # å¢åŠ å»¶è¿Ÿ
            
            print(f"ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡æ’å…¥å®Œæˆï¼æ€»å…±æˆåŠŸæ’å…¥ {total_inserted} æ¡è®°å½•ã€‚")
            
            # æ¸…ç†è¿›åº¦æ–‡ä»¶
            if os.path.exists(progress_file):
                os.remove(progress_file)
                print("ğŸ§¹ å·²æ¸…ç†è¿›åº¦æ–‡ä»¶")
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡æ’å…¥æ•°æ®åº“æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            print("ğŸ’¡ å»ºè®®ï¼šå¦‚æœè¿˜æ˜¯è¶…æ—¶ï¼Œå¯ä»¥å°è¯•å‡å° batch_size çš„å€¼")
            # ä¿å­˜å½“å‰è¿›åº¦ï¼Œä»¥ä¾¿ä¸‹æ¬¡ç»§ç»­
            save_progress(progress_file, {
                "processed_nodes": list(processed_nodes),
                "processed_relationships": list(processed_relationships),
                "documents": documents_to_insert,
                "current_phase": "insert"
            })
    else:
        print("æ²¡æœ‰æ‰¾åˆ°å¯ä»¥å¤„ç†çš„èŠ‚ç‚¹æ•°æ®ã€‚")

# æ‰§è¡Œä¸»å‡½æ•°
if __name__ == '__main__':
    script_dir = os.path.dirname(os.path.abspath(__file__))
    json_path = os.path.join(script_dir, "unreal_engine_graph.json")
    
    if os.path.exists(json_path):
        migrate_data(json_path)
    else:
        print(f"é”™è¯¯ï¼šåœ¨æŒ‡å®šè·¯å¾„æ‰¾ä¸åˆ°æ–‡ä»¶ '{json_path}'ã€‚") 